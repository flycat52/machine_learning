{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>trade</td>\n",
       "      <td>brazil anti inflation plan limps to anniversar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>crude</td>\n",
       "      <td>diamond shamrock dia cuts crude prices diamond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>crude</td>\n",
       "      <td>opec may have to meet to firm prices analysts ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>crude</td>\n",
       "      <td>texaco canada cuts crude prices canadian cts b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>crude</td>\n",
       "      <td>texaco canada txc lowers crude postings texaco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Class                                               Text\n",
       "15  trade  brazil anti inflation plan limps to anniversar...\n",
       "43  crude  diamond shamrock dia cuts crude prices diamond...\n",
       "55  crude  opec may have to meet to firm prices analysts ...\n",
       "76  crude  texaco canada cuts crude prices canadian cts b...\n",
       "77  crude  texaco canada txc lowers crude postings texaco..."
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this workshop we perform document clustering using sklearn\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# We are using the subnews dataset that we used last week. \n",
    "# The \"Class\" labels here are only used for sanity check of the clusters found later.\n",
    "# Remember, in actual use of document clustering, the documents DON'T come with labeled classes.\n",
    "# It's unsupervised learning.\n",
    "\n",
    "import pandas as pd\n",
    "news=pd.read_table('r8-train-all-terms.txt',header=None,names = [\"Class\", \"Text\"])\n",
    "subnews=news[(news.Class==\"trade\")| (news.Class=='crude')|(news.Class=='money-fx') ]\n",
    "subnews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's use the similar preprocessing we used last week.\n",
    "# The output of each document is a list of tokens.\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "mystopwords=stopwords.words(\"English\") + ['one', 'become', 'get', 'make', 'take']\n",
    "WNlemma = nltk.WordNetLemmatizer()\n",
    "\n",
    "def pre_process(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens=[ WNlemma.lemmatize(t.lower()) for t in tokens]\n",
    "    tokens=[ t for t in tokens if t not in mystopwords]\n",
    "    tokens = [ t for t in tokens if len(t) >= 3 ]\n",
    "    text_after_process=\" \".join(tokens)\n",
    "    return(text_after_process)\n",
    "\n",
    "# Apply preprocessing to every document in the training set.\n",
    "text = subnews['Text']\n",
    "toks = text.apply(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(710, 2500)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tfidf matrix\n",
    "vectorizer = TfidfVectorizer(max_df=0.7, max_features=2500,\n",
    "                             min_df=3, stop_words=mystopwords,\n",
    "                             use_idf=True)\n",
    "X = vectorizer.fit_transform(toks)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use SVD to reduce dimensions\n",
    "svd = TruncatedSVD(300)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "X_lsa = lsa.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance of the SVD step: 85%\n"
     ]
    }
   ],
   "source": [
    "# Check how much variance is explained\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.6 ms, sys: 908 µs, total: 30.5 ms\n",
      "Wall time: 16.9 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=3, n_init=1, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the actual clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km3 = KMeans(n_clusters=3, init='k-means++', max_iter=100, n_init=1)\n",
    "%time km3.fit(X_lsa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity for 3 clusters: 0.602\n"
     ]
    }
   ],
   "source": [
    "# How do we know the clustering result is good or not?\n",
    "# If we have labels available, we can use this to derive how coherent the clusters are.\n",
    "# Homogeneity: each cluster contains only members of a single class\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "labels = subnews['Class']\n",
    "print(\"Homogeneity for 3 clusters: %0.3f\" % metrics.homogeneity_score(labels, km3.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.6 ms, sys: 1.29 ms, total: 17.9 ms\n",
      "Wall time: 9.1 ms\n",
      "CPU times: user 54.6 ms, sys: 488 µs, total: 55.1 ms\n",
      "Wall time: 27.8 ms\n",
      "CPU times: user 33.4 ms, sys: 121 µs, total: 33.6 ms\n",
      "Wall time: 16.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "    n_clusters=5, n_init=1, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's try some other K values to compare their metrics\n",
    "km2 = KMeans(n_clusters=2, init='k-means++', max_iter=100, n_init=1)\n",
    "%time km2.fit(X_lsa)\n",
    "\n",
    "km4 = KMeans(n_clusters=4, init='k-means++', max_iter=100, n_init=1)\n",
    "%time km4.fit(X_lsa)\n",
    "\n",
    "km5 = KMeans(n_clusters=5, init='k-means++', max_iter=100, n_init=1)\n",
    "%time km5.fit(X_lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity for 2 clusters: 0.543\n",
      "Homogeneity for 4 clusters: 0.832\n",
      "Homogeneity for 5 clusters: 0.615\n"
     ]
    }
   ],
   "source": [
    "# What are their homogeneity scores?\n",
    "print(\"Homogeneity for 2 clusters: %0.3f\" % metrics.homogeneity_score(labels, km2.labels_))\n",
    "print(\"Homogeneity for 4 clusters: %0.3f\" % metrics.homogeneity_score(labels, km4.labels_))\n",
    "print(\"Homogeneity for 5 clusters: %0.3f\" % metrics.homogeneity_score(labels, km5.labels_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completeness for 2 clusters: 0.922\n",
      "Completeness for 3 clusters: 0.751\n",
      "Completeness for 4 clusters: 0.715\n",
      "Completeness for 5 clusters: 0.488\n"
     ]
    }
   ],
   "source": [
    "# Completeness: all members of a given class are assigned to the same cluster\n",
    "\n",
    "print(\"Completeness for 2 clusters: %0.3f\" % metrics.completeness_score(labels, km2.labels_))\n",
    "print(\"Completeness for 3 clusters: %0.3f\" % metrics.completeness_score(labels, km3.labels_))\n",
    "print(\"Completeness for 4 clusters: %0.3f\" % metrics.completeness_score(labels, km4.labels_))\n",
    "print(\"Completeness for 5 clusters: %0.3f\" % metrics.completeness_score(labels, km5.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Coefficient for 2 clusters: 0.031\n",
      "Silhouette Coefficient for 3 clusters: 0.050\n",
      "Silhouette Coefficient for 4 clusters: 0.055\n",
      "Silhouette Coefficient for 5 clusters: 0.054\n"
     ]
    }
   ],
   "source": [
    "# Silhouette: more similar within clusters, more distant between clusters\n",
    "# The higher the better\n",
    "print(\"Silhouette Coefficient for 2 clusters: %0.3f\"\n",
    "      % metrics.silhouette_score(X_lsa, km2.labels_))\n",
    "print(\"Silhouette Coefficient for 3 clusters: %0.3f\"\n",
    "      % metrics.silhouette_score(X_lsa, km3.labels_))\n",
    "print(\"Silhouette Coefficient for 4 clusters: %0.3f\"\n",
    "      % metrics.silhouette_score(X_lsa, km4.labels_))\n",
    "print(\"Silhouette Coefficient for 5 clusters: %0.3f\"\n",
    "      % metrics.silhouette_score(X_lsa, km5.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We still need to see the more representative words for each cluster to understand them.\n",
    "\n",
    "def print_terms(cm, num):\n",
    "    original_space_centroids = svd.inverse_transform(cm.cluster_centers_)\n",
    "    order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i in range(num):\n",
    "        print(\"Cluster %d:\" % i, end='')\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' %s' % terms[ind], end='')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: oil crude price opec barrel dlrs mln bpd company ecuador\n",
      "Cluster 1: trade billion bank stg japan mln market exchange rate money\n"
     ]
    }
   ],
   "source": [
    "# Print the terms for the 2-cluster model\n",
    "print_terms(km2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: stg mln money bank england market revised assistance shortage forecast\n",
      "Cluster 1: trade billion japan bank exchange dollar currency rate japanese would\n",
      "Cluster 2: oil crude price opec barrel dlrs mln bpd company ecuador\n"
     ]
    }
   ],
   "source": [
    "print_terms(km3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: bank currency exchange rate dollar fed say baker paris treasury\n",
      "Cluster 1: stg mln money bank england market revised assistance shortage forecast\n",
      "Cluster 2: oil crude price opec barrel dlrs mln bpd company ecuador\n",
      "Cluster 3: trade japan billion japanese surplus export year import deficit would\n"
     ]
    }
   ],
   "source": [
    "print_terms(km4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: crude bbl price dlrs raise wti posting posted effective raised\n",
      "Cluster 1: stg mln money bank england market revised assistance shortage forecast\n",
      "Cluster 2: oil opec barrel mln price bpd crude ecuador saudi energy\n",
      "Cluster 3: trade japan bank exchange rate currency dollar japanese market baker\n",
      "Cluster 4: billion trade dlrs fed deficit surplus january bill export february\n"
     ]
    }
   ],
   "source": [
    "print_terms(km5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's map the cluster label to the categories to see where is the confusion\n",
    "\n",
    "dict = {2: 'crude', 0: 'money-fx', 1: 'trade'}\n",
    "cluster_labels = [ dict[c] for c in km3.labels_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[245   0   0]\n",
      " [  1  52   0]\n",
      " [  7 154 251]]\n",
      "0.771830985915\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      crude       0.97      1.00      0.98       245\n",
      "   money-fx       0.25      0.98      0.40        53\n",
      "      trade       1.00      0.61      0.76       412\n",
      "\n",
      "avg / total       0.93      0.77      0.81       710\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(metrics.confusion_matrix(cluster_labels, labels))\n",
    "print(np.mean(cluster_labels == labels) )\n",
    "print(metrics.classification_report(cluster_labels, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
